{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWlz3/dNmlCNHDrRnGB13D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/UK-07/Math-of-Machine-Learning/blob/main/Vectorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorization\n",
        "\n",
        "Machine Learning involves a lot of fundamental operations that are repeated over parameters, over examples and across layers. While these operations are implemented with nested loops for illustration, that is a very ineffcient operation and does not scale for any significant real-world application. In practical implementations these operations are implemented as Linear Algebra operations, leveraging libraries that are much more optimized for practical applications. \\\n",
        "\\\n",
        "Here will gradually build up these matrix operations to understand the underlying mathematical operations. We will start with a linear-regression operation on a single variable for a single example and proceed to build a Multi-Layer Perceptron that trains over multiple examples."
      ],
      "metadata": {
        "id": "a9nP_w5QjcYb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2-Variable Linear Regression\n",
        "The most fundamental Machine Learning operation is linear regression, which solves a linear equation. \\\n",
        "\\\n",
        "\\begin{align}\n",
        "\\mathbf{z} = w_1 x_1 + w_2 x_2 + b\n",
        "\\end{align}\n",
        "\n",
        "Here:\n",
        "* $\\vec{w}$ = $[w_1, w_2]$ are the model weights learned over training\n",
        "* $\\vec{x}$ = $[x_1, x_2]$ are the features of the input data for a single example\n",
        "* $b$ is the _bias_\n",
        "\n",
        "Without vectorization, these are implemented in the following snippet."
      ],
      "metadata": {
        "id": "kZc2DuA-pcpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "np.random.seed(42)  # Set seed value for repeatability.\n",
        "x_bar = np.random.randint(0, 10, 2) # Features of the input example.\n",
        "w_bar = np.random.randint(0, 10, 2) # Weights of the models, typically trained over multiple examples.\n",
        "b = np.random.randint(0, 10) # Bias value.\n",
        "\n",
        "print(f\"Input features x: {x_bar}\")\n",
        "print(f\"Model weights w: {w_bar}\")\n",
        "print(f\"Model Bias b: {b}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF8DrS_uorZh",
        "outputId": "059d55f0-1b9e-40a9-b3aa-a99a97195122"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input features X: [6 3]\n",
            "Model weights W: [7 4]\n",
            "Model Bias b: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.perf_counter_ns()\n",
        "# Solving for `z`.\n",
        "z = 0\n",
        "for x, w in zip(x_bar, w_bar):\n",
        "  z += x * w\n",
        "z += b\n",
        "end = time.perf_counter_ns()"
      ],
      "metadata": {
        "id": "YAZJwPk1oxTF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Solving for z (for-loop): {z} in time {end-start}ns\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOAOpRYW0bMv",
        "outputId": "63668d4f-4587-41da-c6d5-a768495343fc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solvin for z (for-loop): 60 in time 224936ns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above product can also be represented as a dot product of 2 vectors $\\vec{\\mathbf{x}}=\\begin{bmatrix} x_1 & x_2 \\end{bmatrix}$ and $\\vec{w}=\\begin{bmatrix} w_1 & w_2 \\end{bmatrix}$: \\\n",
        "\\\n",
        "\\begin{align}\n",
        "\\vec{\\mathbf{w}} \\cdot \\vec{\\mathbf{x}} &= \\begin{bmatrix} w_1 & w_2 \\end{bmatrix} \\cdot \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix}\n",
        "\\\\\n",
        "&= \\sum_{i} w_i x_i \\\\\n",
        "&= w_1 x_1 + w_2 x_2\n",
        "\\end{align} \\\n",
        "\n",
        "Notice the arrow on $\\vec{\\mathbf{x}}$ and $\\vec{\\mathbf{w}}$ indicating they are vectors. \\\n",
        "\\\n",
        "For a single example, this is dot product between two vectors of size $(2, 1)$, resulting in a scalar value for the dot product. This scales for similarly for vectors of all lengths.\\\n",
        "\\\n",
        "We implement this using the dot-product operation in the `numpy` library."
      ],
      "metadata": {
        "id": "D4exeZRxugCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gtpGmxx42WW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.perf_counter_ns()\n",
        "z = np.dot(w_bar, x_bar) + b\n",
        "end = time.perf_counter_ns()"
      ],
      "metadata": {
        "id": "mjl62_hmuewi"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Solving for z (dot-product): {z} in time {end-start}ns\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMFOqRiC0ezg",
        "outputId": "674c08a7-d36d-4143-f1e0-13244bc0f0f5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving for z (dot-product): 60 in time 172189ns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the difference in runtime is a few nano-seconds and it may seem insignificant. However, at this stage, we're dealing with 2 variables on a single example."
      ],
      "metadata": {
        "id": "OOLojwl7FjBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiple Examples\n",
        "Scaling up this example, when we have multiple examples, there are multiple vectors, one $\\vec{\\mathbf{x}_{i}}$ for each example $i$. In this case we have our weights and input features setup as follows: \\\n",
        "$\\vec{\\mathbf{w}} = \\begin{bmatrix} w_1 \\ldots w_n \\end{bmatrix}$ and $\\mathbf{X} = \\begin{bmatrix} \\vec{x^{(1)}} \\ldots \\vec{x^{(m)}} \\end{bmatrix}$ for $m$ examples with $n$ features each. \\\n",
        "\\\n",
        "Notice the capitalization of $\\mathbf{X}$ indicating that it is a matrix. More specifically: \\\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{X} \\, =\n",
        "  \\begin{bmatrix}\n",
        "  {x}_{1}^{(1)} & {x}_{1}^{(2)} & \\ldots & {x}_{1}^{(m)} \\\\\n",
        "  {x}_{2}^{(1)} & {x}_{2}^{(2)} & \\ldots & {x}_{2}^{(m)} \\\\\n",
        "  \\vdots        & \\vdots        & \\ddots & \\vdots \\\\\n",
        "  {x}_{n}^{(1)} & {x}_{n}^{(2)} & \\ldots & {x}_{n}^{(m)}\n",
        "  \\end{bmatrix}\n",
        "\\end{align} \\\n",
        "\n",
        "To solve for $z_i$ for all examples $i$, our equation now becomes: \\\n",
        "\n",
        "\\begin{align}\n",
        "\\vec{\\mathbf{z}} \\, &= \\, \\vec{\\mathbf{w}} \\cdot \\mathbf{X}\n",
        "\\\\\n",
        "&= \\, \\vec{\\mathbf{w}} \\cdot \\begin{bmatrix} \\vec{x^{(1)}} & \\ldots & \\vec{x^{(m)}} \\end{bmatrix}\n",
        "\\\\\n",
        "&=\n",
        "  \\begin{bmatrix}\n",
        "    \\vec{\\mathbf{w}} \\cdot \\vec{x^{(1)}} & \\ldots & \\vec{\\mathbf{w}} \\cdot \\vec{x^{(m)}}\n",
        "  \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "\\\n",
        "Notice the arrow on top of $\\vec{\\mathbf{z}}$ indicating that it is now a vector $\\begin{bmatrix} z^{(1)} & \\ldots & z^{(m)} \\end{bmatrix}$ containing the result for each example. \\\n",
        "\n",
        "We will now implement this step with nested loops and using a dot product."
      ],
      "metadata": {
        "id": "CeXyUF9l3Ckn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We're now using values from 0-1 rather than integers.\n",
        "_EXAMPLES = 9\n",
        "_FEATURES = 7\n",
        "X = np.random.rand(_EXAMPLES, _FEATURES)\n",
        "w_bar = np.random.rand(_FEATURES)  # Weight matrix is still a vector of weights corresponding each feature.\n",
        "b = np.random.rand()\n",
        "\n",
        "# Set a 3-decimal point precision for cleaner printing.\n",
        "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
        "\n",
        "print(f\"Input features x: \\n{X}\")\n",
        "print(f\"Model weights w: \\n{w_bar}\")\n",
        "print(f\"Model Bias b: {b:,.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoXr3kAM3CB-",
        "outputId": "5cd69fe9-262f-45a2-b5b3-923d05317dc2"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input features x: \n",
            "[[ 0.267  0.120  0.230  0.106  0.822  0.139  0.747]\n",
            " [ 0.048  0.949  0.711  0.007  0.764  0.514  0.779]\n",
            " [ 0.522  0.641  0.786  0.405  0.003  0.850  0.381]\n",
            " [ 0.496  0.008  0.492  0.986  0.278  0.395  0.687]\n",
            " [ 0.815  0.417  0.310  0.938  0.677  0.119  0.069]\n",
            " [ 0.484  0.220  0.254  0.878  0.090  0.715  0.271]\n",
            " [ 0.760  0.722  0.007  0.509  0.139  0.866  0.872]\n",
            " [ 0.418  0.444  0.303  0.717  0.359  0.268  0.432]\n",
            " [ 0.091  0.747  0.013  0.644  0.183  0.232  0.636]]\n",
            "Model weights w: \n",
            "[ 0.993  0.778  0.226  0.176  0.866  0.444  0.137]\n",
            "Model Bias b: 0.862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.perf_counter_ns()\n",
        "# Solving for `z`.\n",
        "z_bar = np.empty(_EXAMPLES)\n",
        "for i, x_bar in enumerate(X):\n",
        "  z = 0\n",
        "  for x, w in zip(x_bar, w_bar):\n",
        "    z += x * w\n",
        "  z += b\n",
        "  z_bar[i] = z\n",
        "end = time.perf_counter_ns()"
      ],
      "metadata": {
        "id": "Oq-cCWlFzP22"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Solving for z (for-loop): {z_bar} in time {end-start}ns\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pR72rrfWD53E",
        "outputId": "b9b982c4-a13f-40f3-ef46-3c15c4aa1d0b"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving for z (for-loop): [ 2.167  2.806  2.561  2.156  2.880  2.158  2.895  2.307  1.999] in time 377484ns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.perf_counter_ns()\n",
        "Z = np.dot(w_bar, X.T) + b\n",
        "end = time.perf_counter_ns()"
      ],
      "metadata": {
        "id": "gpN_3z0rElr5"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Solving for z (dot-product): {z_bar} in time {end-start}ns\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DUGAMhJFQEs",
        "outputId": "9a184d7e-2710-41ef-e72f-280c744ca0d8"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving for z (dot-product): [ 2.167  2.806  2.561  2.156  2.880  2.158  2.895  2.307  1.999] in time 1342508ns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now starting to see the performance difference. While the absolute difference is on the scale of a few milliseconds, that is now a few orders of magnitude higher than the single example scenario. We can modify the values of `_EXAMPLES` and `_FEATURES` to see how execution time for both the operations have different growth curves."
      ],
      "metadata": {
        "id": "4fIfW4PdF7zZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Broadcasting\n",
        "An interesting note about the above dot-product operation is the addition of the bias. Note how bias is a scalar but the result of `np.dot(X, w_bar)` is a vector of dimension $m$. How does this operation not fail? \\\n",
        "\\\n",
        "That is because of _broadcasting_, which is a process by which `numpy` makes copies of the variable `b` and replicates it to match the dimension of the vector it is being added to. In this case, since the dimension of the output for `np.dot(X, w_bar)` is $m$, `numpy` will replicate `b` into an $m$-dimensional vector prior to the addition operation. Broadcasting operation can also occur for matrix and vector pairs and more generally for tensors of any dimension. \\\n",
        "\\\n",
        "**Note:** The concept of making copies and replication is for illustration only. In practice, this implementation would be terribly inefficient."
      ],
      "metadata": {
        "id": "8FuiJ7RXGzqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Layer Perceptron\n",
        "\n",
        "We're now ready to look at the vectorization for a Multi-Layer Perceptron. In this scenario, we will have multiple neurons in each layer and each neuron will have it's own weights for the input features. This implies there will be a $n$-dimensional weight vector of each neuron $k$:\n",
        "\n",
        "\\begin{align}\n",
        "  \\mathbf{W} \\, =\n",
        "    \\begin{bmatrix}\n",
        "      w_{1, 1} & \\ldots & w_{1, n} \\\\\n",
        "      w_{2, 1} & \\ldots & w_{2, n} \\\\\n",
        "      \\vdots & \\ddots & \\vdots \\\\\n",
        "      w_{k, 1} & \\ldots & w_{k, n}\n",
        "    \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "To solve for $z$ for all the examples across all neurons, we now have:\n",
        "\n",
        "\\begin{align}\n",
        "  \\mathbf{Z}_{(k \\, \\times \\, m)} \\,\n",
        "  &= \\, \\mathbf{W}_{(k \\, \\times \\, n)} \\cdot \\mathbf{X}_{(n \\, \\times \\, m)}\n",
        "  \\\\\n",
        "  &= \\begin{bmatrix}\n",
        "      w_{1, 1} & \\ldots & w_{1, n} \\\\\n",
        "      w_{2, 1} & \\ldots & w_{2, n} \\\\\n",
        "      \\vdots & \\ddots & \\vdots \\\\\n",
        "      w_{k, 1} & \\ldots & w_{k, n}\n",
        "    \\end{bmatrix}\n",
        "    \\cdot\n",
        "    \\begin{bmatrix}\n",
        "      {x}_{1}^{(1)} & {x}_{1}^{(2)} & \\ldots & {x}_{1}^{(m)} \\\\\n",
        "      {x}_{2}^{(1)} & {x}_{2}^{(2)} & \\ldots & {x}_{2}^{(m)} \\\\\n",
        "      \\vdots        & \\vdots        & \\ddots & \\vdots \\\\\n",
        "      {x}_{n}^{(1)} & {x}_{n}^{(2)} & \\ldots & {x}_{n}^{(m)}\n",
        "    \\end{bmatrix}\n",
        "  \\\\\n",
        "  \\\\\n",
        "  &= \\begin{bmatrix}\n",
        "    \\sum_{i}w_{1, i} \\cdot x_{i}^{(1)} & \\ldots & \\sum_{i}w_{1, i} \\cdot x_{i}^{(m)} \\\\\n",
        "    \\sum_{i}w_{2, i} \\cdot x_{i}^{(1)} & \\ldots & \\sum_{i}w_{2, i} \\cdot x_{i}^{(m)} \\\\\n",
        "    \\vdots & \\ddots & \\vdots \\\\\n",
        "    \\sum_{i}w_{k, i} \\cdot x_{i}^{(1)} & \\ldots & \\sum_{i}w_{k, i} \\cdot x_{i}^{(m)} \\\\\n",
        "  \\end{bmatrix}\n",
        "\\end{align}\n",
        "\n",
        "\\\n",
        "\n",
        "**Notice** the typeface for $\\mathbf{Z}$ indicates it is now a $(k \\times m)$ matrix - One output for each of the $k$ neurons across $m$ examples.\n",
        "\\\n",
        "Again, we will solve these using nested loops and matrix-multiplication to see the runtime difference."
      ],
      "metadata": {
        "id": "PZY--3KpIV7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_EXAMPLES = 9\n",
        "_FEATURES = 7\n",
        "_NODES = 5\n",
        "\n",
        "X = np.random.rand(_EXAMPLES, _FEATURES)\n",
        "W = np.random.rand(_NODES, _FEATURES)  # *NOTE*: Pay attention to the dimensions. Reason will be clear later.\n",
        "b_bar = np.random.rand(_NODES)  # Each neuron also has it's own bias value so bias is now a vector.\n",
        "\n",
        "# Set a 3-decimal point precision for cleaner printing.\n",
        "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
        "\n",
        "print(f\"Input features x: \\n{X}\")\n",
        "print(f\"Model weights w: \\n{W}\")\n",
        "print(f\"Model Bias b: \\n{b_bar}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjRZrWUvFUym",
        "outputId": "fac45597-af92-4e01-af5d-650dc795c2b0"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input features x: \n",
            "[[ 0.082  0.218  0.250  0.832  0.463  0.487  0.935]\n",
            " [ 0.637  0.899  0.457  0.149  0.930  0.459  0.182]\n",
            " [ 0.302  0.051  0.468  0.853  0.934  0.933  0.125]\n",
            " [ 0.382  0.028  0.855  0.359  0.631  0.715  0.430]\n",
            " [ 0.306  0.962  0.964  0.289  0.480  0.973  0.005]\n",
            " [ 0.413  0.100  0.509  0.025  0.407  0.221  0.533]\n",
            " [ 0.996  0.801  0.846  0.635  0.042  0.207  0.925]\n",
            " [ 0.366  0.104  0.702  0.299  0.269  0.998  0.858]\n",
            " [ 0.408  0.980  0.650  0.742  0.032  0.122  0.633]]\n",
            "Model weights w: \n",
            "[[ 0.899  0.624  0.134  0.116  0.844  0.139  0.520]\n",
            " [ 0.037  0.081  0.672  0.689  0.341  0.216  0.814]\n",
            " [ 0.217  0.542  0.968  0.148  0.941  0.183  0.567]\n",
            " [ 0.377  0.965  0.274  0.400  0.983  0.536  0.503]\n",
            " [ 0.258  0.701  0.014  0.050  0.599  0.342  0.953]]\n",
            "Model Bias b: \n",
            "[ 0.903  0.714  0.818  0.667  0.126]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.perf_counter_ns()\n",
        "# Solving for `z`.\n",
        "Z = np.empty((_NODES, _EXAMPLES))\n",
        "\n",
        "for i, w_bar in enumerate(W):  # Loop over the 1st axis of W, which nodes.\n",
        "  for j, x_bar in enumerate(X):  # Loop over the 1st axis of X which is examples.\n",
        "    z = 0\n",
        "    for x, w in zip(x_bar, w_bar):\n",
        "      z += x * w\n",
        "      z += b[i]  # Bias is a node level entity.\n",
        "    Z[i][j] = z\n",
        "end = time.perf_counter_ns()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szF4sKWNQWqh",
        "outputId": "3599e5e4-9b81-4a86-cc07-ee11425d1940"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-93-87f9da57af82>:11: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  Z[i][j] = z\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Solving for z (for-loop): {Z} in time {end-start}ns\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDtAZGRzScTy",
        "outputId": "8e076c2f-71b5-4ffb-d418-5cdf3be60a46"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving for z (for-loop): [[ 4.669  5.540  4.833  4.758  4.966  4.541  5.512  4.720  4.909]\n",
            " [ 1.889  1.173  1.642  1.661  1.417  1.106  2.022  1.808  1.697]\n",
            " [ 8.004  8.599  8.241  8.396  8.645  7.814  8.613  8.230  8.218]\n",
            " [ 8.661  9.376  8.946  8.601  9.252  8.021  9.083  8.613  8.821]\n",
            " [ 3.110  3.252  2.716  2.737  2.963  2.569  3.395  3.068  3.059]] in time 1763813ns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Numpy vectors of shape (X,) act wierd with broadcasting operations.\n",
        "# We delibrately set the shape of the vector to (X, 1) to indicate its a\n",
        "# column vector.\n",
        "print(f\"Before reshape: {b_bar.shape}\")\n",
        "b_bar = b_bar.reshape((_NODES, 1))\n",
        "print(f\"After reshape: {b_bar.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zt316nE-Sk_b",
        "outputId": "2b12228c-cf75-407a-9251-4d375abdedf4"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before reshape: (5,)\n",
            "After reshape: (5, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.perf_counter_ns()\n",
        "b_bar.reshape(_NODES, -1)\n",
        "Z = np.dot(W, X.T) + b_bar\n",
        "end = time.perf_counter_ns()"
      ],
      "metadata": {
        "id": "cKBfiE3XShpm"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Solving for z (dot-product): {z_bar} in time {end-start}ns\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmWcFuegSzvx",
        "outputId": "b776ff17-eea8-47fc-8ba0-7628fd9d0e8c"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving for z (dot-product): [ 2.167  2.806  2.561  2.156  2.880  2.158  2.895  2.307  1.999] in time 1399143ns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the difference is starting to widen. As the dimensions of our data and model increase, so will this difference."
      ],
      "metadata": {
        "id": "bo9U_knXUOBz"
      }
    }
  ]
}